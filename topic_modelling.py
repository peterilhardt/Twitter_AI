# Functions for Topic Modelling of text data (e.g. LSA, NMF, LDA)


def display_wordcloud(data, title = None, file = None, bg_color = 'black', 
					  stopwords = None, max_words = 200, max_font_size = 40, 
					  scale = 3, random_state = None, prefer_horizontal = 0.9):
    """
    Generates a WordCloud from text data using the wordcloud module. It will 
    display 'max_words' words from the input text as a WordCloud image, which 
    randomly displays the most frequently occurring words in a horizontal or 
    vertical arrangement, making more frequent words larger in size. It is a 
    useful visualization to quickly show the most common words in text data. 
    
    Parameters
    ----------
    data: single string OR list of strings OR other object with multiple strings
     	  (e.g. array)
        The text data to generate a WordCloud from
    title: string, default: None
        Optional title for the image
    file: string, default: None
        Filename for optionally saving the WordCloud as image output
    bg_color: string, default: 'black'
        Background color for the WordCloud (typically 'white' or 'black')
    stopwords: list or set of strings, default: None
        Words to avoid including in the WordCloud
    max_words: int, default: 200
        Maximum number of words to include in the WordCloud
    max_font_size: int, default: 40
        Maximum font size to make a word in the WordCloud
    scale: int, default: 3
        Scaling factor between computation and drawing
    random_state: int, default: None
    prefer_horizontal: float in range(0, 1), default: 0.9
        Fraction of words to display horizontally instead of vertically
        
    Returns
    -------
    matplotlib image
        WordCloud of the input text data 
    """

    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    wordcloud = WordCloud(
	    background_color = bg_color,
	    stopwords = stopwords,
	    max_words = max_words,
	    max_font_size = max_font_size, 
	    scale = scale,
	    random_state = random_state,
	    prefer_horizontal = prefer_horizontal
    ).generate(str(data))

    fig = plt.figure(1, figsize = (12, 12))
    plt.axis('off')
    if title: 
	    fig.suptitle(title, fontsize = 20, fontweight = 'bold')
	    fig.subplots_adjust(top = 1.38)

    plt.imshow(wordcloud)

    if file:
	    plt.savefig(file, dpi = 100, bbox_inches = 'tight')
    plt.show()
    plt.close()


def doc_term_matrix(text, vectorizer = 'CV', stop_words = 'english'):
	"""
	Generates a document-term matrix for a series of documents using either
	a Count Vectorizer or TF-IDF Vectorizer. A document-term matrix has the
	documents as rows and all words that appear in a document as columns.

	Parameters
	----------
	text: Series
		The documents to be converted to a document-term matrix
	vectorizer: string: 'CV' or 'TFIDF', default: 'CV'
		Whether CountVectorizer or TfidfVectorizer should be used, respectively
	stop_words: string, default: 'english'
		Optional stopwords to exclude from the document-term matrix

	Returns
	-------
	DataFrame
		The document-term matrix
	"""

	from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
	import pandas as pd

	if vectorizer == 'CV':
	    vec = CountVectorizer(stop_words = stop_words)
	elif vectorizer == 'TFIDF':
	    vec = TfidfVectorizer(stop_words = stop_words)

	fit = vec.fit_transform(text)
	df = pd.DataFrame(fit.toarray(), columns = vec.get_feature_names())
	return df


def display_topics_svd(model_fit, terms, num_top_words, topics = None):
	"""
	Display the 'num_top_words' top words for each topic generated by latent
	semantic analysis (LSA) or non-negative matrix factorization (NMF). 
	Displayed as readable text output.

	Parameters
	----------
	model_fit: TruncatedSVD or NMF model fit
		Result of applying fit_transform to document-term matrix
	terms: array or list
		The words comprising the 'terms' in the document-term matrix 
	num_top_words: int
		The number of top words to display for each topic
	topics: list, default: None
		Optional list of pre-defined topic names

	Returns
	-------
	structured text output
		Shows each topic and the num_top_words top words for that topic below
	"""

    for ind, topic in enumerate(model_fit.components_):
        
        if not topics or not topics[ind]:
            print("\nTopic ", ind + 1, sep = '')
        else:
            print("\nTopic: '", topics[ind], "'", sep = '')
        
        print(", ".join([terms[i] for i in topic.argsort()\
        	  [:(-num_top_words - 1):(-1)]]))


def display_topics_lda(text, num_topics, passes, vectorizer = 'CV', \
                       ngram_range = (1, 1), binary = False, num_words = 10):
	"""
	Generate a document-term matrix from text data input, fit a latent dirichlet
	allocation (LDA) model to the matrix, and return the top words and weights
	for each topic generated by the model. 

	Parameters
	----------
	text: array or list
		The text data input
	num_topics: int
		The number of topics to derive from the data
	passes: int
		The number of LDA fit cycles to complete
	vectorizer: string: 'CV' or 'TFIDF', default: 'CV'
		Whether to use CountVectorizer or TfidfVectorizer, respectively
	ngram_range: 2-tuple, default: (1, 1)
		The range of n-gram lengths to consider for vectorization
	binary: boolean, default: False
		Whether to generate indicator variables for each term rather than counts
	num_words: int, default: 10
		The number of words/weights to display for each topic

	Returns
	-------
	structured text output
		Top num_words words and weights for each of the num_topics topics
	"""

	from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
	from gensim import models, matutils

	if vectorizer == 'CV':
	    vec = CountVectorizer(stop_words = 'english', ngram_range = ngram_range, 
	    				  	  binary = binary)
	elif vectorizer == 'TFIDF':
	    vec = TfidfVectorizer(stop_words = 'english', ngram_range = ngram_range, 
	    				  	  binary = binary)

	vec.fit(text)
	doc_term = vec.transform(text).transpose()
	corpus = matutils.Sparse2Corpus(doc_term)
	id2word = dict((v, k) for k, v in vec.vocabulary_.items())
	lda = models.LdaModel(corpus = corpus, num_topics = num_topics, 
						  id2word = id2word, passes = passes)
	return lda.print_topics(num_topics = num_topics, num_words = num_words)

